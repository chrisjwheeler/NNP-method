{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will write the particle filtering class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existing particle filtering class from the real world functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "\n",
    "import equinox as eqx\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ParticleFilter:\n",
    "    \"\"\"\n",
    "    A frozen dataclass implementing a particle filter with support for different sampling\n",
    "    and weighting strategies based on time periods (minute, overnight, weekend, other).\n",
    "    \n",
    "    This particle filter can handle different market conditions by switching between\n",
    "    different proposal distributions and weight functions based on a tau_id indicator.\n",
    "    \n",
    "    Attributes:\n",
    "        min_sample_fn (callable): Sampling function for minute-level data\n",
    "        min_weight_fn (callable): Weight function for minute-level data\n",
    "        overnight_sample_fn (callable): Sampling function for overnight periods\n",
    "        overnight_weight_fn (callable): Weight function for overnight periods\n",
    "        weekend_sample_fn (callable, optional): Sampling function for weekends. \n",
    "            Defaults to overnight_sample_fn if None\n",
    "        weekend_weight_fn (callable, optional): Weight function for weekends.\n",
    "            Defaults to overnight_weight_fn if None\n",
    "        other_sample_fn (callable, optional): Sampling function for other periods.\n",
    "            Defaults to weekend_sample_fn if None\n",
    "        other_weight_fn (callable, optional): Weight function for other periods.\n",
    "            Defaults to weekend_weight_fn if None\n",
    "        final_reweight (callable): Final reweighting function. Defaults to identity\n",
    "        ESS_COND (float): Effective sample size threshold for resampling. Defaults to 0.5\n",
    "        N_PARTICLES (int): Number of particles in the filter. Defaults to 2500\n",
    "        Y_LOOK_FORWARD (int): Number of future observations to look ahead. Defaults to 0\n",
    "        switch_resampling_in_step (bool): Whether to resample before or after sampling.\n",
    "            Defaults to False (after sampling)\n",
    "        needs_final_reweight (bool): Whether final reweighting is needed. Defaults to False\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    min_sample_fn: callable\n",
    "    min_weight_fn: callable\n",
    "    overnight_sample_fn: callable\n",
    "    overnight_weight_fn: callable  \n",
    "\n",
    "    weekend_sample_fn: callable = None\n",
    "    weekend_weight_fn: callable = None\n",
    "    other_sample_fn: callable = None\n",
    "    other_weight_fn: callable = None\n",
    "\n",
    "    final_reweight: callable = lambda *args: args\n",
    "\n",
    "    ESS_COND: float = 0.5\n",
    "    N_PARTICLES: int = 2500\n",
    "    Y_LOOK_FORWARD: int = 0\n",
    "    switch_resampling_in_step: bool = False\n",
    "    needs_final_reweight: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Post-initialization setup for the particle filter.\n",
    "        \n",
    "        Sets default functions for weekend and other periods if not provided,\n",
    "        and initializes diagnostic names for tracking filter performance.\n",
    "        \"\"\"\n",
    "        # Workaround for frozen=True: use object.__setattr__\n",
    "        if self.weekend_sample_fn is None:\n",
    "            object.__setattr__(self, 'weekend_sample_fn', self.overnight_sample_fn)\n",
    "        if self.weekend_weight_fn is None:\n",
    "            object.__setattr__(self, 'weekend_weight_fn', self.overnight_weight_fn)\n",
    "        if self.other_sample_fn is None:\n",
    "            object.__setattr__(self, 'other_sample_fn', self.weekend_sample_fn)\n",
    "        if self.other_weight_fn is None:\n",
    "            object.__setattr__(self, 'other_weight_fn', self.weekend_weight_fn)\n",
    "\n",
    "        object.__setattr__(self, 'diagnostic_names', ['ess', 'resample_flag', 'normalised_entropy', 'marginal_likelihood'])\n",
    "\n",
    "    @staticmethod    \n",
    "    def multinomial_resample(subkey, weights, N_particles):\n",
    "        \"\"\"\n",
    "        Perform multinomial resampling of particles based on their weights.\n",
    "        \n",
    "        Args:\n",
    "            subkey: JAX random key for sampling\n",
    "            weights: Array of particle weights (must sum to 1)\n",
    "            N_particles: Number of particles to resample\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (resample_indices, log_weights, resample_flag)\n",
    "                - resample_indices: Indices of resampled particles\n",
    "                - log_weights: Log weights after resampling (uniform)\n",
    "                - resample_flag: 1 if resampling occurred, 0 otherwise\n",
    "        \"\"\"\n",
    "        resample_indices = jax.random.choice(\n",
    "            subkey, N_particles, p=weights, shape=(N_particles,)\n",
    "        )\n",
    "        log_weights = jnp.log(1 / N_particles) * jnp.ones_like(weights)\n",
    "        return resample_indices, log_weights, 1\n",
    "\n",
    "    @staticmethod\n",
    "    def get_online_metric_args(log_weights, unormalised_log_weights, n_particles):\n",
    "        \"\"\"\n",
    "        Calculate online diagnostic metrics for the particle filter.\n",
    "        \n",
    "        Args:\n",
    "            log_weights: Current log weights of particles\n",
    "            unormalised_log_weights: Unnormalized log weights before normalization\n",
    "            n_particles: Number of particles\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (entropy, marginal_likelihood)\n",
    "                - entropy: Normalized entropy of the particle weights\n",
    "                - marginal_likelihood: Marginal likelihood estimate\n",
    "        \"\"\"\n",
    "        entropy = jnp.sum(jnp.exp(log_weights) * log_weights) + jnp.log(n_particles)\n",
    "        marginal_likelihood = jnp.sum(jnp.exp(unormalised_log_weights))\n",
    "        \n",
    "        return entropy, marginal_likelihood\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_ess(log_weights):\n",
    "        \"\"\"\n",
    "        Calculate the effective sample size (ESS) of the particle weights.\n",
    "        \n",
    "        Args:\n",
    "            log_weights: Log weights of particles\n",
    "            \n",
    "        Returns:\n",
    "            float: Effective sample size\n",
    "        \"\"\"\n",
    "        return 1.0 / jnp.exp(jax.scipy.special.logsumexp(2 * log_weights))\n",
    "    \n",
    "    def combined_sample_fn(self, tau_id, subkey, particles, Y_array, idt):\n",
    "        \"\"\"\n",
    "        Select and apply the appropriate sampling function based on tau_id.\n",
    "        \n",
    "        Args:\n",
    "            tau_id: Time period identifier (0=minute, 1=overnight, 2=weekend, 3=other)\n",
    "            subkey: JAX random key for sampling\n",
    "            particles: Current particle states\n",
    "            Y_array: Observation array\n",
    "            idt: Time index\n",
    "            \n",
    "        Returns:\n",
    "            array: New sampled particles\n",
    "        \"\"\"\n",
    "        return jax.lax.switch(\n",
    "            tau_id,\n",
    "            [\n",
    "                lambda: self.min_sample_fn(subkey, particles, Y_array, idt),\n",
    "                lambda: self.overnight_sample_fn(subkey, particles, Y_array, idt),\n",
    "                lambda: self.weekend_sample_fn(subkey, particles, Y_array, idt),\n",
    "                lambda: self.other_sample_fn(subkey, particles, Y_array, idt),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def combined_weight_fn(self, tau_id, sampled_particles, particles, Y_array, idt):\n",
    "        \"\"\"\n",
    "        Select and apply the appropriate weight function based on tau_id.\n",
    "        \n",
    "        Args:\n",
    "            tau_id: Time period identifier (0=minute, 1=overnight, 2=weekend, 3=other)\n",
    "            sampled_particles: Newly sampled particles\n",
    "            particles: Previous particle states\n",
    "            Y_array: Observation array\n",
    "            idt: Time index\n",
    "            \n",
    "        Returns:\n",
    "            array: Weight updates for the particles\n",
    "        \"\"\"\n",
    "        return jax.lax.switch(\n",
    "            tau_id,\n",
    "            [\n",
    "                lambda: self.min_weight_fn(sampled_particles, particles, Y_array, idt),\n",
    "                lambda: self.overnight_weight_fn(sampled_particles, particles, Y_array, idt),\n",
    "                lambda: self.weekend_weight_fn(sampled_particles, particles, Y_array, idt),\n",
    "                lambda: self.other_weight_fn(sampled_particles, particles, Y_array, idt),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def simulate(\n",
    "            self,\n",
    "            key: jax.random.PRNGKey,\n",
    "            initial_particles: jnp.ndarray, \n",
    "            initial_log_weights: jnp.ndarray,\n",
    "            Y_array: jnp.ndarray,\n",
    "            tau_id_array: jnp.ndarray,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the particle filter simulation over the entire time series.\n",
    "        \n",
    "        This method implements a sequential Monte Carlo particle filter that can\n",
    "        switch between different proposal distributions and weight functions based\n",
    "        on the tau_id_array. It performs resampling when the effective sample size\n",
    "        falls below the threshold.\n",
    "        \n",
    "        Args:\n",
    "            key: JAX random key for the simulation\n",
    "            initial_particles: Initial particle states\n",
    "            initial_log_weights: Initial log weights of particles\n",
    "            Y_array: Array of observations\n",
    "            tau_id_array: Array of time period identifiers for each time step\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (final_particles, final_log_weights, filter_diagnostics)\n",
    "                - final_particles: Final particle states\n",
    "                - final_log_weights: Final log weights\n",
    "                - filter_diagnostics: Dictionary containing diagnostic metrics\n",
    "                    (ess, resample_flag, normalised_entropy, marginal_likelihood)\n",
    "                    \n",
    "        Raises:\n",
    "            AssertionError: If Y_array shape doesn't match tau_id_array shape + Y_LOOK_FORWARD\n",
    "            NotImplementedError: If final reweighting is requested but not implemented\n",
    "        \"\"\"\n",
    "        \n",
    "        assert Y_array.shape[0] == tau_id_array.shape[0] + self.Y_LOOK_FORWARD, \"The Y_array is not the correct shape relative to the tau_id_array and the Y_LOOK_FORWARD\"\n",
    "\n",
    "        def particle_filter_step(carry, time_slice):\n",
    "            tau_id, idt = time_slice\n",
    "            key, Y_array, particles, log_weights = carry\n",
    "            \n",
    "            if self.switch_resampling_in_step:\n",
    "                key, subkey = jax.random.split(key)\n",
    "\n",
    "                ess = self.calculate_ess(log_weights)\n",
    "                particle_indices, log_weights, resample_flag = jax.lax.cond(\n",
    "                    ess/self.N_PARTICLES < self.ESS_COND,\n",
    "                    lambda k, log_w: self.multinomial_resample(k, jnp.exp(log_w), self.N_PARTICLES),\n",
    "                    lambda _, log_w: (jnp.arange(self.N_PARTICLES), log_w, 0),\n",
    "                    *(subkey, log_weights)\n",
    "                )\n",
    "                \n",
    "                particles = sampled_particles[particle_indices]\n",
    "           \n",
    "\n",
    "            # 2. Classic particle filter logic\n",
    "\n",
    "             # Sample new particles using the proposal distribution\n",
    "            key, subkey = jax.random.split(key)\n",
    "            sampled_particles = self.combined_sample_fn(tau_id, subkey, particles, Y_array, idt)\n",
    "\n",
    "            # Update particle weights\n",
    "            weight_update = self.combined_weight_fn(tau_id, sampled_particles, particles, Y_array, idt)\n",
    "            unormalised_log_weights = log_weights + weight_update\n",
    "            log_weights = unormalised_log_weights - jsp.special.logsumexp(unormalised_log_weights)\n",
    "\n",
    "            online_metric_args = self.get_online_metric_args(log_weights, unormalised_log_weights, self.N_PARTICLES)\n",
    "\n",
    "            if not self.switch_resampling_in_step:\n",
    "                key, subkey = jax.random.split(key)\n",
    "\n",
    "                ess = self.calculate_ess(log_weights)\n",
    "                particle_indices, log_weights, resample_flag = jax.lax.cond(\n",
    "                    ess/self.N_PARTICLES < self.ESS_COND,\n",
    "                    lambda k, log_w: self.multinomial_resample(k, jnp.exp(log_w), self.N_PARTICLES),\n",
    "                    lambda _, log_w: (jnp.arange(self.N_PARTICLES), log_w, 0),\n",
    "                    *(subkey, log_weights)\n",
    "                )\n",
    "                particles = sampled_particles[particle_indices]\n",
    "\n",
    "            return (key, Y_array, particles, log_weights), (ess, resample_flag, *online_metric_args,)\n",
    "        \n",
    "        # 1. Run scan.\n",
    "\n",
    "        Y_idt_half_array = jnp.arange(tau_id_array.shape[0])\n",
    "        Y_idt_array = jnp.column_stack((tau_id_array, Y_idt_half_array))\n",
    "\n",
    "        # Run filter\n",
    "        final_carry, online_metric_list = jax.lax.scan(\n",
    "            particle_filter_step, \n",
    "            (key, Y_array, initial_particles, initial_log_weights),\n",
    "            Y_idt_array\n",
    "        )\n",
    "\n",
    "        # 2. Process output of scan.\n",
    "        _, _, final_particles, final_log_weights = final_carry\n",
    "        filter_diagnostics = {diagnostic_name: val for diagnostic_name, val in zip(self.diagnostic_names, online_metric_list)}\n",
    "\n",
    "        # AUX final reweight\n",
    "        if self.needs_final_reweight:\n",
    "            raise NotImplementedError(\"Final reweighting not implemented\")\n",
    "            # We are going to have issues as we need the previous particles, which we dont have without writing them into scan or terminating one early.\n",
    "            # When re-writing terminate one step early.\n",
    "\n",
    "        return final_particles, final_log_weights, filter_diagnostics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existing Simulate Forward Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import equinox as eqx\n",
    "\n",
    "import pf_functions.constants as const\n",
    "\n",
    "\n",
    "def f_from_noise(last_val, noise):\n",
    "    \"\"\"\n",
    "    Evolve the hidden state using the stochastic volatility model.\n",
    "    \n",
    "    Args:\n",
    "        last_val: Previous hidden state value\n",
    "        noise: Random noise for the evolution\n",
    "        \n",
    "    Returns:\n",
    "        next_val: Next hidden state value\n",
    "    \"\"\"\n",
    "    means = last_val + const.TAU * const.KAPPA * (const.X_BAR - last_val)\n",
    "    std = jnp.sqrt(const.TAU) * const.SIGMA_SIGMA\n",
    "    next_val = noise * std + means\n",
    "    return next_val\n",
    "\n",
    "def overnight_f_from_noise(last_val, noise):\n",
    "    \"\"\"\n",
    "    Evolve the hidden state using the stochastic volatility model.\n",
    "    \n",
    "    Args:\n",
    "        last_val: Previous hidden state value\n",
    "        noise: Random noise for the evolution\n",
    "        \n",
    "    Returns:\n",
    "        next_val: Next hidden state value\n",
    "    \"\"\"\n",
    "    overnight_TAU_adjust = 8 * 60 * 0.4/0.6\n",
    "    means = last_val + const.TAU * overnight_TAU_adjust * const.KAPPA * (const.X_BAR - last_val)\n",
    "    std = jnp.sqrt(const.TAU * overnight_TAU_adjust) * const.SIGMA_SIGMA\n",
    "    next_val = noise * std + means\n",
    "    return next_val\n",
    "\n",
    "def g_from_total_noise(prev_particle, all_particles, total_noise):\n",
    "    \"\"\"\n",
    "    Generate observations from hidden states using the observation model.\n",
    "    \n",
    "    Args:\n",
    "        all_particles: Hidden state particles\n",
    "        total_noise: Random noise for observations\n",
    "        \n",
    "    Returns:\n",
    "        observations: Generated observations\n",
    "    \"\"\"\n",
    "    y_means = -0.5 * const.TAU * jnp.exp(2 * all_particles)\n",
    "    y_stds = jnp.sqrt(const.TAU) * jnp.exp(all_particles)\n",
    "    return total_noise * y_stds + y_means\n",
    "\n",
    "def skew_g_from_total_noise(prev_particle, particle, noise):\n",
    "    # Mean reversion term: E[X_t | X_{t-1}] = X_{t-1} + κτ(X̄ - X_{t-1})\n",
    "    mean_reversion_term = prev_particle + const.KAPPA * const.TAU * (const.X_BAR - prev_particle)\n",
    "\n",
    "    # Mean: μ_Y = -0.5τ exp(2X_t) + ρ(exp(X_t)/σ²)(X_t - E[X_t | X_{t-1}])\n",
    "    skew_total_mean = -0.5 * const.TAU * jnp.exp(2 * particle) + const.RHO * (jnp.exp(particle) / const.SIGMA_SIGMA) * (particle - mean_reversion_term)\n",
    "    \n",
    "    # Variance: σ²_Y = τ exp(2X_t)(1 - ρ²)\n",
    "    skew_total_var = const.TAU * jnp.exp(2 * particle) * (1 - const.RHO**2)\n",
    "\n",
    "    return skew_total_mean + noise * jnp.sqrt(skew_total_var)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SimulatingForward:\n",
    "    \"\"\"\n",
    "    A class for simulating forward particle paths and evaluating their likelihood.s\n",
    "    \n",
    "    This class implements forward simulation of particle paths from initial particle states\n",
    "    and evaluates the likelihood of observed data using kernel density estimation on\n",
    "    realized volatility metrics.\n",
    "    \n",
    "    Attributes:\n",
    "        min_f_from_noise: Function to evolve state for minute-level periods\n",
    "        overnight_f_from_noise: Function to evolve state for overnight periods  \n",
    "        observation_from_noise: Function to generate observations from state and noise\n",
    "        weekend_f_from_noise: Function to evolve state for weekend periods (defaults to overnight)\n",
    "        other_f_from_noise: Function to evolve state for other periods (defaults to weekend)\n",
    "        N_SAMPLES: Number of forward simulation samples (default: 2500)\n",
    "    \"\"\"\n",
    "    min_f_from_noise: callable\n",
    "    overnight_f_from_noise: callable\n",
    "    observation_from_noise: callable\n",
    "    weekend_f_from_noise: callable = None\n",
    "    other_f_from_noise: callable = None\n",
    "    N_SAMPLES: int = 2500\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Post-initialization setup for the forward simulator.\n",
    "        \n",
    "        Sets default functions for weekend and other periods if not provided.\n",
    "        \"\"\"\n",
    "        # Workaround for frozen=True: use object.__setattr__\n",
    "        if self.weekend_f_from_noise is None:\n",
    "            object.__setattr__(self, 'weekend_f_from_noise', self.overnight_f_from_noise)\n",
    "        if self.other_f_from_noise is None:\n",
    "            object.__setattr__(self, 'other_f_from_noise', self.weekend_f_from_noise)\n",
    "\n",
    "    def evaluation_metrics_from_paths(self, sampled_forward_particles, sampled_observation_paths, \n",
    "                                    Y_array, tau_id_array, total_tau_time):\n",
    "        \"\"\"\n",
    "        Calculate likelihood of observed data given simulated forward paths.\n",
    "        \n",
    "        Uses kernel density estimation on realized volatility to estimate the likelihood\n",
    "        of the target realized volatility given the distribution of simulated realized volatilities.\n",
    "        \n",
    "        Args:\n",
    "            sampled_forward_particles: Simulated particle states\n",
    "            sampled_observation_paths: Simulated observation paths\n",
    "            Y_array: Target observation array\n",
    "            tau_id_array: Time period identifiers\n",
    "            total_tau_time: Total time duration in minutes\n",
    "            \n",
    "        Returns:\n",
    "            float: Estimated likelihood of target realized volatility\n",
    "        \"\"\"\n",
    "        # Calculate realized volatility with time normalization\n",
    "        tau_normalized_constant = 1 / total_tau_time\n",
    "        target_realized_vol = tau_normalized_constant * jnp.sum(Y_array**2)\n",
    "        sampled_realized_vol = tau_normalized_constant * jnp.sum(sampled_observation_paths**2, axis=1)\n",
    "\n",
    "        # Estimate likelihood using kernel density estimation\n",
    "        kde = jsp.stats.gaussian_kde(sampled_realized_vol, bw_method=\"scott\")\n",
    "        likelihood = kde.logpdf(target_realized_vol)\n",
    "    \n",
    "        return likelihood\n",
    "\n",
    "    def combined_f_from_noise(self, tau_id, last_val, noise):\n",
    "        \"\"\"\n",
    "        Select and apply the appropriate state evolution function based on tau_id.\n",
    "        \n",
    "        Args:\n",
    "            tau_id: Time period identifier (0=minute, 1=overnight, 2=weekend, 3=other)\n",
    "            last_val: Previous state value\n",
    "            noise: Random noise for state evolution\n",
    "            \n",
    "        Returns:\n",
    "            array: New state value\n",
    "        \"\"\"\n",
    "        return jax.lax.switch(\n",
    "            tau_id,\n",
    "            [\n",
    "                lambda: self.min_f_from_noise(last_val, noise),\n",
    "                lambda: self.overnight_f_from_noise(last_val, noise),\n",
    "                lambda: self.weekend_f_from_noise(last_val, noise),\n",
    "                lambda: self.other_f_from_noise(last_val, noise),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def simulate_forward(self, key: jax.random.PRNGKey, initial_particles: jnp.ndarray,\n",
    "                        initial_log_weights: jnp.ndarray, Y_array: jnp.ndarray,\n",
    "                        tau_id_array: jnp.ndarray, total_tau_time: float):\n",
    "        \"\"\"\n",
    "        Simulate forward particle paths and evaluate their likelihood.\n",
    "        \n",
    "        This method samples particles from the initial distribution, simulates their\n",
    "        forward evolution, generates corresponding observations, and evaluates the\n",
    "        likelihood of the target observations using realized volatility metrics.\n",
    "        \n",
    "        Args:\n",
    "            key: JAX random key for the simulation\n",
    "            initial_particles: Initial particle states\n",
    "            initial_log_weights: Initial log weights of particles\n",
    "            Y_array: Target observation array\n",
    "            tau_id_array: Time period identifiers for each time step\n",
    "            total_tau_time: Total time duration in minutes for normalization\n",
    "            \n",
    "        Returns:\n",
    "            tuple: ((sampled_forward_particles, sampled_observation_paths), likelihood)\n",
    "                - sampled_forward_particles: Simulated particle state paths\n",
    "                - sampled_observation_paths: Simulated observation paths\n",
    "                - likelihood: Estimated likelihood of target observations\n",
    "        \"\"\"\n",
    "        sample_key, path_key, observation_key = jax.random.split(key, 3)\n",
    "\n",
    "        # Sample starting points from initial particle distribution\n",
    "        starting_point_indices = jax.random.choice(\n",
    "            sample_key, initial_particles.shape[0], (self.N_SAMPLES,), \n",
    "            p=jnp.exp(initial_log_weights)\n",
    "        )\n",
    "        starting_points = initial_particles[starting_point_indices]\n",
    "\n",
    "        # Define state evolution functions\n",
    "        def individual_jump_body(carry, time_slice):\n",
    "            \"\"\"Single step of state evolution\"\"\"\n",
    "            tau_id, noise = time_slice\n",
    "            next_val = self.combined_f_from_noise(tau_id, carry, noise)\n",
    "            return next_val, next_val\n",
    "    \n",
    "        def scan_fn(initial_points, tau_id_array, jump_noise):\n",
    "            \"\"\"Evolve states over multiple time steps\"\"\"\n",
    "            _, hidden_state_evolution = jax.lax.scan(individual_jump_body, initial_points, (tau_id_array, jump_noise))\n",
    "            return hidden_state_evolution\n",
    "\n",
    "        # Generate forward particle paths\n",
    "        jump_noises = jax.random.normal(path_key, (self.N_SAMPLES, Y_array.shape[0]))\n",
    "        sampled_forward_particles = jax.vmap(scan_fn, in_axes=(0, None, 0))(\n",
    "            starting_points, tau_id_array, jump_noises\n",
    "        )\n",
    "\n",
    "        # Generate observations from the particle paths\n",
    "        def observation_jump_body(all_particle_array, time_slice):\n",
    "            \"\"\"Single step of state evolution\"\"\"\n",
    "            idt, noise = time_slice\n",
    "            new_y = self.observation_from_noise(all_particle_array.at[idt-1].get(), all_particle_array.at[idt].get(), noise)\n",
    "            return all_particle_array, new_y\n",
    "\n",
    "        def observation_scan_fn(all_points, jump_noise):\n",
    "            \"\"\"Evolve states over multiple time steps\"\"\"\n",
    "\n",
    "            # Expand the all_points array.\n",
    "            all_points = jnp.concatenate((jnp.expand_dims(all_points[0], axis=0), all_points))\n",
    "\n",
    "            # Create the xs vals, which is a tuple of arrays.\n",
    "            time_slice = jnp.arange(1, all_points.shape[0], dtype=jnp.int32)\n",
    "\n",
    "            _, hidden_state_evolution = jax.lax.scan(observation_jump_body, all_points, (time_slice, jump_noise))\n",
    "            return hidden_state_evolution\n",
    "\n",
    "        observation_noise = jax.random.normal(observation_key, sampled_forward_particles.shape)\n",
    "        sampled_observation_paths = observation_scan_fn(sampled_forward_particles, observation_noise)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        result_metrics = self.evaluation_metrics_from_paths(\n",
    "            sampled_forward_particles, sampled_observation_paths, \n",
    "            Y_array, tau_id_array, total_tau_time\n",
    "        )\n",
    "\n",
    "        return (sampled_forward_particles, sampled_observation_paths), result_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing functions, this is where the large changes will take place as we are no longer going through a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import polars as pl\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pf_functions.constants as const\n",
    "\n",
    "\n",
    "def pre_processing(data_slice, forecast_negative_increments: list, n_particles: int = 2500, particle_initialisation_key: jax.random.PRNGKey = jax.random.key(0)):\n",
    "    \"\"\"\n",
    "    Preprocess financial time series data for particle filtering analysis.\n",
    "    \n",
    "    This function performs three main preprocessing steps:\n",
    "    1. Cleans the dataset by removing unused columns and categorizing time intervals (tau)\n",
    "    2. Creates forecast flags based on specified time horizons before expiry\n",
    "    3. Initializes particles and weights for particle filtering\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_slice : pl.LazyFrame or pl.DataFrame\n",
    "        Input financial time series data containing columns: dt, raw_tau, front_expiry, \n",
    "        front_implied, back_implied, front_tte, back_tte, back_expiry\n",
    "    forecast_negative_increments : list\n",
    "        List of hours before expiry to create forecast flags for. \n",
    "        Each value creates a corresponding {i}_f_flag column\n",
    "    n_particles : int, default=2500\n",
    "        Number of particles to initialize for particle filtering\n",
    "    particle_initialisation_key : jax.random.PRNGKey, default=jax.random.key(0)\n",
    "        Random key for reproducible particle initialization\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (processed_df, (initial_particles, initial_log_weights))\n",
    "        - processed_df: pl.DataFrame with preprocessing applied\n",
    "        - initial_particles: jax.Array of shape (n_particles,) with initial particle states\n",
    "        - initial_log_weights: jax.Array of shape (n_particles,) with initial log weights\n",
    "        \n",
    "    Notes:\n",
    "    ------\n",
    "    - Tau categorization: 0 (< 0.1h), 1 (17.5h), 2 (65.5h), 3 (other)\n",
    "    - Forecast flags are created for each hour in forecast_negative_increments\n",
    "    - Particles are initialized from Normal(X_BAR, SIGMA_SIGMA) distribution\n",
    "    - Log weights are initialized uniformly (-log(n_particles))\n",
    "    \"\"\"\n",
    "    if not isinstance(data_slice, pl.LazyFrame):\n",
    "        data_slice = data_slice.lazy()\n",
    "    \n",
    "    # 1. Drop unused columns and calculate tau values\n",
    "    data_slice = data_slice.drop(['back_implied', 'front_tte', 'back_tte', 'back_expiry'])\n",
    "\n",
    "\n",
    "    data_slice = data_slice.with_columns([\n",
    "        pl.when(pl.col(\"raw_tau\").is_null())\n",
    "          .then(None)\n",
    "        .when(pl.col(\"raw_tau\") < 0.1)          # less than ~6 mins (0.1 hr)\n",
    "          .then(0)\n",
    "        .when(pl.col(\"raw_tau\") == 17.5)\n",
    "          .then(1)\n",
    "        .when(pl.col(\"raw_tau\") == 65.5)\n",
    "          .then(2)\n",
    "        .otherwise(3)\n",
    "        .alias(\"tau_id\")\n",
    "    ])\n",
    "    \n",
    "    data_slice = data_slice.filter(~pl.col(\"log_returns\").is_nan()) # Removing the nan values. We do this after so that the gaps are treated as minutes.\n",
    "\n",
    "    # drop the first row\n",
    "    data_slice = data_slice.slice(1)\n",
    "\n",
    "    data_slice_with_tau = data_slice.collect()\n",
    "\n",
    "    # 2. Create the F_flag columns using forecast_negative_increments\n",
    "    for i, expiry_hour in enumerate(forecast_negative_increments):\n",
    "        max_forecast_times = data_slice_with_tau.select(\n",
    "            (pl.col(\"front_expiry\") - pl.duration(hours=expiry_hour)).alias(\"forecast_time\")\n",
    "        ).unique().to_series()\n",
    "\n",
    "        data_slice_with_tau = data_slice_with_tau.with_columns([\n",
    "            pl.col(\"dt\").is_in(pl.Series(max_forecast_times).implode()).alias(f\"{i}_f_flag\")\n",
    "        ])\n",
    "\n",
    "    f_flag_columns = [f\"{i}_f_flag\" for i in range(len(forecast_negative_increments))]\n",
    "    data_slice_with_tau = data_slice_with_tau.with_columns([\n",
    "        pl.fold(acc=pl.lit(False), function=lambda acc, x: acc | x, exprs=[pl.col(col) for col in f_flag_columns]).alias(\"any_f_flag\")\n",
    "    ])\n",
    "\n",
    "    true_count = data_slice_with_tau.filter(pl.col(\"any_f_flag\") == True).height\n",
    "    print(f\"Number of Forecast Points values in any_f_flag: {true_count}\")\n",
    "    print(f\"Total height of data frame: {data_slice_with_tau.height}\")\n",
    "\n",
    "    # 3. Create the initial particles and weights\n",
    "    initial_particles = jax.random.normal(particle_initialisation_key, (n_particles, )) * const.SIGMA_SIGMA + const.X_BAR\n",
    "    initial_log_weights = -jnp.log(n_particles) * jnp.ones_like(initial_particles)\n",
    "\n",
    "    return data_slice_with_tau, (initial_particles, initial_log_weights)  # processed_df, (initial_particles, initial_weights)\n",
    "\n",
    "\n",
    "def processing(key, particle_filter, processed_data_base, particles_weights_tuple, break_after: int = -1, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Process a time series dataset using a particle filter, segmenting the data at forecast points.\n",
    "    \n",
    "    This function iteratively processes segments of data between forecast points (marked by 'any_f_flag')\n",
    "    using a particle filter. It maintains particle states and weights across segments and collects\n",
    "    diagnostics for each processing step.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    key : jax.random.PRNGKey\n",
    "        Random number generator key for JAX operations\n",
    "    particle_filter : ParticleFilter\n",
    "        Particle filter object with a 'simulate' method that processes data segments\n",
    "    processed_data_base : polars.DataFrame\n",
    "        Preprocessed dataset containing columns: 'any_f_flag', 'tau_id', 'log_returns'\n",
    "    particles_weights_tuple : tuple\n",
    "        Initial particles and log weights as (particles, log_weights)\n",
    "    break_after : int, optional (default=-1)\n",
    "        Number of segments to process before stopping. If -1, processes all segments\n",
    "    verbose : bool, optional (default=True)\n",
    "        Whether to display progress bar and detailed output for each segment\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (diagnostic_from_segment_dict, particle_and_weights_at_flag_idx)\n",
    "        - diagnostic_from_segment_dict: Dictionary mapping segment indices (start, end) to \n",
    "          particle filter diagnostics (ESS, resample flags, marginal likelihoods)\n",
    "        - particle_and_weights_at_flag_idx: Dictionary mapping forecast point indices to \n",
    "          final particle states and weights at those points\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - Segments are defined between consecutive forecast points (where 'any_f_flag' is True)\n",
    "    - The particle filter processes each segment and maintains state continuity\n",
    "    - Log returns include lookahead data (Y_LOOK_FORWARD steps) for the particle filter\n",
    "    - Diagnostics include Effective Sample Size (ESS), resampling flags, and marginal likelihoods\n",
    "    \"\"\"\n",
    "    \n",
    "    diagnostic_from_segment_dict = {}\n",
    "    particle_and_weights_at_flag_idx = {}\n",
    "    \n",
    "    # Get the row indices where any_f_flag is True\n",
    "    flag_mask = processed_data_base['any_f_flag']\n",
    "    flag_indices = [i for i, flag in enumerate(flag_mask) if flag]\n",
    "\n",
    "    last_idx = 0 # First val of pr\n",
    "    total_height = processed_data_base.height\n",
    "\n",
    "    last_particles, last_weights = particles_weights_tuple\n",
    "\n",
    "    # Create progress bar with dataset progress information\n",
    "    \n",
    "    pbar = tqdm(enumerate(flag_indices), total=min(len(flag_indices), break_after)+5, \n",
    "                desc=\"Processing segments\", unit=\"segment\")\n",
    "    \n",
    "\n",
    "    for go, flag_idx in pbar:\n",
    "\n",
    "        # 1. Prepare for the simulation.\n",
    "        segment_to_process = processed_data_base.slice(last_idx + 1, flag_idx - last_idx) # \n",
    "        tau_id_segment = segment_to_process['tau_id'].to_jax()\n",
    "\n",
    "        # Handle log_return slice: add lookahead rows after flag_idx\n",
    "        log_return_steps = flag_idx - last_idx + particle_filter.Y_LOOK_FORWARD\n",
    "\n",
    "        if last_idx + log_return_steps > total_height:\n",
    "            raise NotImplementedError(\"This did in fact come up as an issue.\")\n",
    "        \n",
    "        log_return_segment = processed_data_base.slice(last_idx, log_return_steps)['log_returns'].to_jax()\n",
    "            \n",
    "        # 2. Run the simulation.\n",
    "        key, pf_step_key = jax.random.split(key)\n",
    "        out_particles, out_weights, diagnostics = particle_filter.simulate(\n",
    "            pf_step_key, \n",
    "            last_particles, \n",
    "            last_weights,\n",
    "            log_return_segment,\n",
    "            tau_id_segment\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Segment {go}: Processing from index {last_idx} to {flag_idx} ({(flag_idx/total_height)*100:.1f}% of dataset)\")\n",
    "            print(f\"ESS: {jnp.mean(diagnostics['ess']):.3f}, Resample flag: {jnp.mean(diagnostics['resample_flag']):.3f}\")\n",
    "            print(f\"Marginal likelihood: {jnp.sum(jnp.log(diagnostics['marginal_likelihood'])):.6f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        # 3. Process the output of the simulation.\n",
    "        diagnostic_from_segment_dict[(last_idx + 1, flag_idx + 1)] = diagnostics\n",
    "        particle_and_weights_at_flag_idx[flag_idx] = (out_particles, out_weights) # This is the index of the flag.\n",
    "\n",
    "        # Update the last particles and weights\n",
    "        last_particles = out_particles\n",
    "        last_weights = out_weights\n",
    "        last_idx = flag_idx\n",
    "\n",
    "        if break_after - 1 == go:\n",
    "            break\n",
    "    \n",
    "    return diagnostic_from_segment_dict, particle_and_weights_at_flag_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existing Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import polars as pl\n",
    "\n",
    "from .particle_filter import ParticleFilter\n",
    "from .simulate_forward import SimulatingForward\n",
    "from .processing_functions import pre_processing, processing\n",
    "from .simulate_forward_processing import simulate_forward_processing\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RealWorldWrapper:\n",
    "    \"\"\"\n",
    "    A wrapper class for running particle filter simulations on real-world data.\n",
    "    \n",
    "    This class provides a convenient interface for loading data, preprocessing it,\n",
    "    running particle filtering, and performing forward simulations for forecasting.\n",
    "    \n",
    "    Attributes:\n",
    "        simulate_from_func: Function for performing forward simulations\n",
    "        forecast_negative_increments: List of forecast horizons in hours (negative values)\n",
    "        path_to_data: Path to the parquet data file\n",
    "        n_rows_to_process: Number of rows to load from the data file\n",
    "    \"\"\"\n",
    "    forecast_negative_increments: list\n",
    "    simulate_from_func: SimulatingForward = None\n",
    "    path_to_data: str = r\"C:\\Users\\chris\\OneDrive\\Belgeler\\ParticleFilter\\ParticleFilter\\real_data\\preprocessed_synth_price_data.parquet\"\n",
    "    n_rows_to_process: int = 100000\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize the wrapper by loading and preprocessing data.\"\"\"\n",
    "        object.__setattr__(self, 'loaded_data', pl.read_parquet(\n",
    "            self.path_to_data, \n",
    "            n_rows=self.n_rows_to_process\n",
    "        ).lazy())\n",
    "\n",
    "        # Run the pre-processing\n",
    "        pre_processed_data_base, initial_particles_weights = pre_processing(\n",
    "            self.loaded_data, \n",
    "            self.forecast_negative_increments\n",
    "        )\n",
    "        object.__setattr__(self, 'pre_processed_data_base', pre_processed_data_base)\n",
    "        object.__setattr__(self, 'initial_particles_weights', initial_particles_weights)\n",
    "\n",
    "    def final_step_processing(self, diagnostic_from_segment_dict, raw_fit_metrics):\n",
    "        \"\"\"\n",
    "        Post-process simulation results. Override this method for custom processing.\n",
    "        \n",
    "        Args:\n",
    "            diagnostic_from_segment_dict: Dictionary containing diagnostic results\n",
    "            raw_fit_metrics: Array of fit metrics from simulations\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (processed_diagnostics, processed_fit_metrics)\n",
    "        \"\"\"\n",
    "        return diagnostic_from_segment_dict, raw_fit_metrics\n",
    "\n",
    "    def run_from_particle_filter(self,\n",
    "                               key: jax.random.PRNGKey,\n",
    "                               particle_filter: ParticleFilter,\n",
    "                               simulate_from_func: SimulatingForward = None,\n",
    "                               include_raw_paths: bool = False,\n",
    "                               break_after: int = -1,\n",
    "                               verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Run the complete particle filter pipeline.\n",
    "        \n",
    "        Args:\n",
    "            key: JAX random key for reproducibility\n",
    "            particle_filter: ParticleFilter instance to use\n",
    "            break_after: Maximum number of segments to process (-1 for all)\n",
    "            verbose: Whether to print progress information\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (final_diagnostics, final_fit_metrics)\n",
    "        \"\"\"\n",
    "        # Split key for separate processing and forward simulation\n",
    "        processing_key, forward_key = jax.random.split(key)\n",
    "        \n",
    "        # Run particle filtering\n",
    "        diagnostic_from_segment_dict, particle_and_weights_at_flag_idx = processing(\n",
    "            processing_key, \n",
    "            particle_filter, \n",
    "            self.pre_processed_data_base, \n",
    "            self.initial_particles_weights, \n",
    "            break_after, \n",
    "            verbose\n",
    "        )\n",
    "        \n",
    "        # Run forward simulations\n",
    "\n",
    "        if simulate_from_func is None:\n",
    "            simulate_from_func = self.simulate_from_func\n",
    "\n",
    "        raw_path_dict, raw_fit_metrics = simulate_forward_processing(\n",
    "            forward_key, \n",
    "            simulate_from_func, \n",
    "            self.pre_processed_data_base, \n",
    "            particle_and_weights_at_flag_idx, \n",
    "            self.forecast_negative_increments, \n",
    "            break_after, \n",
    "            verbose\n",
    "        )\n",
    "        \n",
    "        # Apply final processing step\n",
    "        final_diagnostic_from_segment_dict, final_raw_fit_metrics = self.final_step_processing(\n",
    "            diagnostic_from_segment_dict, \n",
    "            raw_fit_metrics\n",
    "        )\n",
    "        if include_raw_paths:\n",
    "            return final_diagnostic_from_segment_dict, final_raw_fit_metrics, raw_path_dict\n",
    "        else:\n",
    "            return final_diagnostic_from_segment_dict, final_raw_fit_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previoulsy started wrapper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pf_functions.config as config\n",
    "config.set_constants()\n",
    "\n",
    "import pf_functions as PF\n",
    "\n",
    "\n",
    "def model_weight_function(model, particles, inputs):\n",
    "    pass\n",
    "\n",
    "def model_sample_from_inputs(model, key, inputs):\n",
    "    pass\n",
    "\n",
    "\n",
    "class NeuralNetworkProcess:\n",
    "    def __init__(self, \n",
    "                 initial_model: eqx.Module,\n",
    "                 data_generation_function: callable,\n",
    "                 training_data_generation_function: callable,\n",
    "                 model_weight_function: callable,\n",
    "                 model_sample_from_inputs: callable, \n",
    "                 state_transition_weight: callable,\n",
    "                 state_observation_weight: callable):\n",
    "\n",
    "\n",
    "        '''Both data generation should be partialed so that we dont handel the function passing inside the class.\n",
    "        state and observation should be for singles. '''\n",
    "        \n",
    "        self.initial_model = initial_model\n",
    "        self.model = initial_model\n",
    "        \n",
    "        self.data_generation_function = data_generation_function\n",
    "        self.training_data_generation_function = training_data_generation_function\n",
    "\n",
    "        self.model_weight_function = model_weight_function\n",
    "        self.model_sample_from_inputs = model_sample_from_inputs\n",
    "\n",
    "        self.state_transition_weight = jax.jit(jax.vmap(state_transition_weight, in_axes=(0, 0, None, None)))\n",
    "        self.state_observation_weight = jax.jit(jax.vmap(state_observation_weight, in_axes=(0, 0, None, None)))\n",
    " \n",
    "\n",
    "    def get_training_data(self, \n",
    "                         training_params: tuple[int, int] = (1000, 5000), \n",
    "                         testing_params: tuple[int, int] = (100, 2500),\n",
    "                         training_key: int = 42,\n",
    "                         testing_key: int = 52,\n",
    "                         verbose: bool = True):\n",
    "        \"\"\"Generate training and testing data with customizable parameters.\n",
    "        \n",
    "        Args:\n",
    "            training_params: (n_batches, batch_size) for training data\n",
    "            testing_params: (n_batches, batch_size) for testing data  \n",
    "            training_key: Random key for training data generation\n",
    "            testing_key: Random key for testing data generation\n",
    "            verbose: Whether to print progress information\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"Generating training data: {training_params[0]} batches of size {training_params[1]}\")\n",
    "            print(f\"Generating testing data: {testing_params[0]} batches of size {testing_params[1]}\")\n",
    "        \n",
    "        data_key = jax.random.key(training_key)\n",
    "        test_data_key = jax.random.key(testing_key)\n",
    "\n",
    "        input_batches, target_batches = self.training_data_generation_function(\n",
    "            data_key, \n",
    "            training_params[0], \n",
    "            training_params[1],\n",
    "        )\n",
    "\n",
    "        inputs = jnp.vstack(input_batches)\n",
    "        targets = jnp.hstack(target_batches)\n",
    "\n",
    "        test_input_batches, test_target_batches = self.training_data_generation_function(\n",
    "            test_data_key, \n",
    "            testing_params[0], \n",
    "            testing_params[1],\n",
    "        )\n",
    "\n",
    "        test_inputs = jnp.vstack(test_input_batches)\n",
    "        test_targets = jnp.hstack(test_target_batches)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Training data shape: inputs {inputs.shape}, targets {targets.shape}\")\n",
    "            print(f\"Testing data shape: inputs {test_inputs.shape}, targets {test_targets.shape}\")\n",
    "\n",
    "        return inputs, targets, test_inputs, test_targets\n",
    "\n",
    "\n",
    "    def train(self, \n",
    "              training_params: tuple[int, int] = (1000, 5000), \n",
    "              testing_params: tuple[int, int] = (100, 2500),\n",
    "              steps: int = 5000,\n",
    "              learning_rate: float = 1e-2,\n",
    "              batch_size: int = 5000,\n",
    "              train_key: int = 20,\n",
    "              eval_frequency: int = 100,\n",
    "              verbose: bool = True):\n",
    "        \"\"\"Train the neural network model with customizable parameters.\n",
    "        \n",
    "        Args:\n",
    "            training_params: (n_batches, batch_size) for training data\n",
    "            testing_params: (n_batches, batch_size) for testing data\n",
    "            steps: Number of training steps\n",
    "            learning_rate: Learning rate for optimizer\n",
    "            batch_size: Batch size for training\n",
    "            train_key: Random key for training\n",
    "            eval_frequency: How often to evaluate on test set\n",
    "            verbose: Whether to print progress information\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"Starting training with {steps} steps, lr={learning_rate}, batch_size={batch_size}\")\n",
    "        \n",
    "        inputs, targets, test_inputs, test_targets = self.get_training_data(\n",
    "            training_params, testing_params, verbose=verbose\n",
    "        )\n",
    "\n",
    "        model = self.model\n",
    "        train_key = jax.random.key(train_key)\n",
    "\n",
    "        optimizer = optax.adam(learning_rate=learning_rate)\n",
    "        opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "        \n",
    "        @eqx.filter_value_and_grad\n",
    "        def loss(model, inputs, z_i):\n",
    "            log_likelihood = self.model_weight_function(model, inputs, z_i)\n",
    "            return -jnp.mean(log_likelihood)\n",
    "\n",
    "        @eqx.filter_jit\n",
    "        def batched_train_step(model, x, y, opt_state, optimizer):\n",
    "            neg_ll, grads = loss(model, x, y)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state)\n",
    "            model = eqx.apply_updates(model, updates)\n",
    "            return model, opt_state, neg_ll\n",
    "\n",
    "        @eqx.filter_jit\n",
    "        def evaluate(model, x, y):\n",
    "            return loss(model, x, y)\n",
    "\n",
    "        losses = []\n",
    "        test_losses = []\n",
    "        loop = tqdm(range(steps))\n",
    "\n",
    "        shuffle_key, train_key = jax.random.split(train_key)\n",
    "        shuffled_ix = jax.random.permutation(shuffle_key, inputs.shape[0])\n",
    "\n",
    "        batch_ix = 0\n",
    "        for step in loop:\n",
    "            if (batch_ix + 1) * batch_size > inputs.shape[0]:\n",
    "                shuffle_key, train_key = jax.random.split(train_key)\n",
    "                shuffled_ix = jax.random.permutation(shuffle_key, inputs.shape[0])\n",
    "                batch_ix = 0\n",
    "            \n",
    "            batch_idx = shuffled_ix[batch_ix * batch_size:(batch_ix + 1) * batch_size]\n",
    "            batch_ix += 1\n",
    "\n",
    "            model, opt_state, neg_ll = batched_train_step(\n",
    "                model, inputs[batch_idx], targets[batch_idx], opt_state, optimizer\n",
    "            )\n",
    "            losses.append(neg_ll)\n",
    "            \n",
    "            # Evaluate on test set periodically\n",
    "            if step % eval_frequency == 0:\n",
    "                test_loss = evaluate(model, test_inputs, test_targets)\n",
    "                test_losses.append(test_loss)\n",
    "                loop.set_postfix({\n",
    "                    'train_loss': f'{neg_ll:.4f}',\n",
    "                    'test_loss': f'{test_loss:.4f}',\n",
    "                    'step': step\n",
    "                })\n",
    "            else:\n",
    "                loop.set_postfix({'train_loss': f'{neg_ll:.4f}', 'step': step})\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nTraining completed!\")\n",
    "            print(f\"Final training loss: {losses[-1]:.4f}\")\n",
    "            if test_losses:\n",
    "                print(f\"Final test loss: {test_losses[-1]:.4f}\")\n",
    "            print(f\"Best test loss: {min(test_losses):.4f}\" if test_losses else \"No test evaluation\")\n",
    "\n",
    "        self.model = model\n",
    "        self.vectorised_model = jax.vmap(model)\n",
    "        self.training_losses = losses\n",
    "        self.test_losses = test_losses\n",
    "\n",
    "    def build_model_inputs(self, prev_particles, Y_array, idt):\n",
    "        new_col = jnp.full((prev_particles.shape[0], 1), Y_array.at[idt].get())\n",
    "        model_input_i = jnp.hstack((prev_particles.reshape(-1, 1), new_col))\n",
    "        return model_input_i\n",
    "\n",
    "\n",
    "    def create_NN_weight_and_sample_from_model_and_proposal_for_skew(self, model):\n",
    "        \"\"\"\n",
    "        Creates weight and sampling functions for a neural network-based particle filter.\n",
    "        \n",
    "        Args:\n",
    "            vectorised_model: Neural network model that outputs means and standard deviations\n",
    "            g_l_single: Single-particle observation likelihood function\n",
    "            f_l_single: Single-particle transition likelihood function\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (weight_function, sampling_function)\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        def NN_weight_fn(particles, prev_particles, Y_array, idt): \n",
    "            \"\"\"\n",
    "            Calculate particle weights using the neural network proposal.\n",
    "            \n",
    "            Args:\n",
    "                Y_i: Current observation\n",
    "                particles: Current state particles\n",
    "                prev_particles: Previous state particles\n",
    "                \n",
    "            Returns:\n",
    "                array: Log weights for each particle\n",
    "            \"\"\"\n",
    "            # Calculate observation likelihood\n",
    "            # I have to change this to be the leverage model, but can I just input the leverage_bootstrap_weight?\n",
    "            g_y_i_from_x_i = self.state_observation_weight(particles, prev_particles, Y_array, idt)\n",
    "            \n",
    "            # Calculate transition likelihood\n",
    "            f_x_i_from_x_prev_i = self.state_transition_weight(particles, prev_particles, Y_array, idt)\n",
    "            \n",
    "            # Calculate proposal likelihood\n",
    "            inputs = self.build_model_inputs(prev_particles, Y_array, idt)\n",
    "            q_x_i_from_x_prev_i_y_i = self.model_weight_function(model, particles, inputs)\n",
    "\n",
    "            # Return log weight: log(g) + log(f) - log(q)\n",
    "            return g_y_i_from_x_i + f_x_i_from_x_prev_i - q_x_i_from_x_prev_i_y_i\n",
    "    \n",
    "        def NN_sample_fn(subkey, prev_particles, Y_array, idt):\n",
    "            \"\"\"\n",
    "            Sample new particles using the neural network proposal.\n",
    "            \n",
    "            Args:\n",
    "                subkey: JAX random key\n",
    "                particles: Current state particles\n",
    "                Y_i: Current observation\n",
    "                \n",
    "            Returns:\n",
    "                array: New sampled particles\n",
    "            \"\"\"\n",
    "            inputs = self.build_model_inputs(prev_particles, Y_array, idt)\n",
    "            new_particles = self.model_sample_from_inputs(model, subkey, inputs)\n",
    "\n",
    "            return new_particles\n",
    "        \n",
    "        return NN_sample_fn, NN_weight_fn\n",
    "            \n",
    "\n",
    "    def get_particle_filter(self, N_particles: int,\n",
    "        input_initial_sample_fn: callable = None,\n",
    "        input_intial_weight_fn: callable = None,\n",
    "        input_resample_fn: callable = None,\n",
    "        vmap: bool = True):\n",
    "\n",
    "        \"\"\"\n",
    "        Creates a neural network-based particle filter from a provided model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : eqx.Module\n",
    "            The neural network model to use for the particle filter\n",
    "        N_particles : int\n",
    "            Number of particles to use in the filter\n",
    "        vmap : bool, optional\n",
    "            Whether to vectorize the filter, by default True\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        callable\n",
    "            A particle filter function that uses the provided neural network model\n",
    "        \"\"\"\n",
    "\n",
    "        if input_initial_sample_fn is None:\n",
    "            chosen_initial_sample_fn = PF.particle_filters.initial_sample_fn\n",
    "        if input_intial_weight_fn is None:\n",
    "            chosen_intial_weight_fn = PF.particle_filters.intial_weight_fn\n",
    "        if input_resample_fn is None:\n",
    "            chosen_resample_fn = PF.particle_filters.multinomial_resample\n",
    "\n",
    "        vectorised_model = jax.vmap(self.model)\n",
    "        \n",
    "        sample_fn, weight_fn = self.create_NN_weight_and_sample_from_model_and_proposal(\n",
    "            vectorised_model, \n",
    "        )\n",
    "\n",
    "        return PF.particle_filters.create_particle_filter(sample_fn, weight_fn, chosen_initial_sample_fn, chosen_intial_weight_fn, chosen_resample_fn, N_particles, vmap)    \n",
    "\n",
    "    def evaluate_model(self, N_particles: int):\n",
    "        # Generate data\n",
    "\n",
    "        # Load the model particle filter. \n",
    "        \n",
    "\n",
    "        # Load the bootstrap particle filter. \n",
    "\n",
    "        # Use Simple plots to \n",
    "\n",
    "\n",
    "    def evaluate_model_and_show(self, **kwargs ):\n",
    "        eval = self.evaluate_model(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ParticleFilter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
